<!DOCTYPE html>
<html lang="en-US">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>cyberd</title>
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<link rel="shortcut icon" type="image/png" href="images/favicon.png">

	
	
	<link rel="stylesheet" href="css/style.css" type="text/css" media="all">
	


	
	<link href="https://fonts.googleapis.com/css?family=Play:400,300,100,700" rel="stylesheet" type='text/css'>


</head>

<body>


<main class="wrapper">

	
	<header class="header">

		
		<div class="mobile-bar visible-sm visible-xs">
			<div class="hamburger-menu">
				  <div class="bar"></div>	
			</div>
		</div>


		<div class="logo_cyberd">
			<p><a href='https://github.com/cybercongress/cyberd' target="_blank" class="logolink"><img src="images/logo-cyberd-2.png" alt="image"></a></p>
		</div>

		<nav class="main-nav">
			<ul class="navigation">
				
				<li><a href="#Glossary">Glossary</a></li>
				<li><a href="#Abstract">Abstract</a></li>
				<li><a href="#Introduction_to_web3">Introduction to web3</a></li>
				<li><a href="#On_adversarial_examples_problem">On adversarial examples problem</a></li>
				<li><a href="#Cyber_protocol_at_euler">Cyber protocol at euler</a></li>
				<li><a href="#Knowledge_graph">Knowledge graph</a></li>
				<li><a href="#Cyberlinks">Cyberlinks</a></li>
				<li><a href="#Notion_of_consensus_computer">Notion of consensus computer</a></li>
				<li><a href="#Relevance_machine">Relevance machine</a></li>
				<li><a href="#cyber•Rank">cyber•Rank</a></li>
				<li><a href="#Proof_of_relevance">Proof of relevance</a></li>
				<li><a href="#Speed_and_scalability">Speed and scalability</a></li>
				<li><a href="#Implementation_in_a_browser">Implementation in a browser</a></li>
				<li><a href="#From_Inception_to_Genesis">From Inception to Genesis</a></li>
				<li><a href="#Validators_incentive">Validators incentive</a></li>
				<li><a href="#Satoshi_Lottery">Satoshi Lottery</a></li>
				<li><a href="#Inception">Inception</a></li>
				<li><a href="#Possible_applications">Possible applications</a></li>
				<li><a href="#Economic_protection_is_smith">Economic protection is smith</a></li>
				<li><a href="#Ability_to_evolve_is_darwin">Ability to evolve is darwin</a></li>
				<li><a href="#Turing_is_about_computing_more">turing is about computing more</a></li>
				<li><a href="#In_a_search_for_equilibria_is_nash">In a search for equilibria is nash</a></li>
				<li><a href="#On_faster_evolution_at_weiner">On faster evolution at weiner</a></li>
				<li><a href="#Genesis_is_secure_as_merkle">Genesis is secure as merkle</a></li>
				<li><a href="#Conclusion">Conclusion</a></li>
				<li><a href="#References">References</a></li>
			</ul>
		</nav>
	</header>
	

	
	<div class="main-content">

			
			

			
				<section class="contact">
					<div class="section-header">

						<div class='imgone'></div>
				
						<div>
							<p class="textimg"><span>/A search<br> consensus computer<br> for web3</span></p>
						</div>
				
						<div class="contacte">



							<p>
								<a class='linkcolor2' href="https://cybercongress.ai" target="_blank">cyber•Congress:</a>
								<a class='linkcolor' href="https://github.com/xhipster" target="_blank">@xhipster,</a>
								<a class='linkcolor' href="https://github.com/litvintech" target="_blank">@litvintech,</a>
								<a class='linkcolor' href="https://github.com/hleb-albau" target="_blank">@hleb-albau,</a>
								<a class='linkcolor' href="https://github.com/arturalbov" target="_blank">@arturalbov,</a>
								<a class='linkcolor' href="https://github.com/belya" target="_blank">@belya</a>
							</p>

							<p>Notes on <a class='linkcolor' href="https://github.com/cybercongress/cyberd/releases/tag/v0.1.0" target="_blank">euler</a> release of cyber : // protocol <a class='linkcolor' href="https://github.com/cybercongress/cyberd" target="_blank">reference implementation</a> using Go.</p>
						</div>
	
						<h2 id="Glossary"><span>&#8260; </span>&nbsp;Glossary</h2>
						<div class='glossary'>
							<ul>
								<li>cyb:</li>
								<li>- nick. a friendly software robot who helps you explore universes</li>
							</ul><br>
						
							<ul>
								<li>cyber:</li>
								<li>- noun. a superintelligent network computer for answers</li>
								<li>- verb. to do something intelligent, to be very smart</li>
							</ul><br>

							<ul>
								<li>cyber://</li>
								<li>- web3 protocol for computing answers and knowledge exchange</li>
							</ul><br>
												
							<ul>
								<li>CYB:</li>
								<li>- ticker. transferable token expressing will to become smarter</li>
							</ul><br>
						
							<ul>
								<li>CYBER:</li>
								<li>- ticker. non-transferable token expressing intelligence</li>
							</ul><br>

							<ul>
								<li>CBD:</li>
								<li>- ticker. ERC-20 proto token representing substance from which CYB emerge</li>
							</ul><br>

							<ul>
								<li>cyberlink:</li>
								<li>- link type. expressing connection from one link to another as link-x.link-y</li>
							</ul>

	
						</div>
						
						<div class="abstract">
							<h2 id="Abstract" class="wow pulse" data-wow-duration="1s" data-wow-offset="100"><span>&#8260; </span>&nbsp;Abstract</h2>
							<p>
								A consensus computer allows computing of provably relevant answers without opinionated blackbox intermediaries such as
								Google, Youtube, Amazon or Facebook. Stateless content-addressable peer-to-peer communication networks such as IPFS and
								stateful consensus computers such as Ethereum provide part of the solution, but there are at least three problems
								associated with implementation. Of course, the first problem is the subjective nature of relevance. The second problem
								is that it is hard to scale consensus computer for a huge knowledge graph. The third problem is that the quality of
								such a knowledge graph will suffer from different attack surfaces such as sybil, selfish behaviour of interacting
								agents. In this paper, we (1) define a protocol for provable consensus computing of relevance between IPFS objects
								based on Tendermint consensus of cyber•rank computed on GPU, (2) discuss implementation details and (3) design
								distribution and incentive scheme based on our experience. We believe the minimalistic architecture of the protocol is
								critical for the formation of a network of domain-specific knowledge consensus computers. As a result of our work some
								applications never existed before emerge. We expand the work including our vision on features we expect to work up to
								Genesis.
							</p>
						</div>	
					</div>
				</section>
				



				
				<section id="Introduction_to_web3" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Introduction to web3</h2>

						<p>
							Original protocols of the Internet such as TCP/IP, DNS, URL, and HTTPS brought a web into the point where it is now.
							Along with all the benefits they have created they brought more problem to the table. Globality being a vital property
							of the web since inception is under real threat. The speed of connections degrades with network grow and from
							ubiquitous government interventions into privacy and security of web users. One property, not evident in the beginning,
							become important with everyday usage of the Internet: its ability to exchange permanent hyperlinks thus they <a class="linkcolor" href="https://ipfs.io/ipfs/QmNhaUrhM7KcWzFYdBeyskoNyihrpHvUEBQnaddwPZigcN" target="_blank">would not
							break after time has passed</a>. Reliance on "one at a time ISP" architecture allows governments effectively censor
							packets. It is the last straw in a conventional web stack for every engineer who is concerned about the future of our
							children.
						</p>
						<br>
						<p>
							Other properties while being not so critical are very desirable: offline and real-time. Average internet user being
							offline must have the ability to work with the state it has and after acquiring connection being able to sync with
							global state and continue to verify state's validity in realtime while having a connection. Now, these properties
							offered on the app level while such properties must be integrated into lower level protocols.
						</p>
						<br>
						<p>
							The emergence of a <a class="linkcolor" href='https://github.com/w3f/Web3-wiki/wiki' target="_blank">web3 stack</a> creates an opportunity for a new kind of Internet. We call it web3. It has a promise to
							remove problems of a conventional protocol stack and add to the web better speed and more accessible connection.
							However, as usual in a story with a new stack, new problems emerge. One of such problem is general-purpose search.
							Existing general-purpose search engines are restrictive centralized databases everybody forced to trust. These search
							engines were designed primarily for client-server architecture based on TCP/IP, DNS, URL and HTTPS protocols. Web3
							creates a challenge and opportunity for a search engine based on developing technologies and specifically designed for
							them. Surprisingly the permission-less blockchain architecture itself allows organizing general purpose search engine
							in a way inaccessible for previous architectures.
						</p>

					</div>
				</section>


				<section id="On_adversarial_examples_problem" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;On adversarial examples problem</h2>
						<p>
							<a class="linkcolor" href="https://ipfs.io/ipfs/QmeS4LjoL1iMNRGuyYSx78RAtubTT2bioSGnsvoaupcHR6" target="_blank">Conventional architecture of search engines</a> where one entity process and rank all the shit suffers from one hard but
							the particular problem that still has not been solved even by brilliant Google scientists:<a class="linkcolor" href="https://ipfs.io/ipfs/QmNrAFz34SLqkzhSg4wAYYJeokfJU5hBEpkT4hPRi226y9" target="_blank">adversarial examples
							problem</a>. The problem Google acknowledge is that it is rather hard to algorithmically reason either this particular
							sample is adversarial or not independently on how cool the learning technology is. Obviously, a cryptoeconomic approach
							can change beneficiaries in this game effectively removing possible sybil attack vectors and removing the necessity to
							make a decision on example crawling and meaning extraction from one entity to the whole world. Learning sybil-resistant
							model will probably lead to orders of magnitude more predictive results.
						</p>

					</div>

				</section>
				



				
				<section id="Cyber_protocol_at_euler" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Cyber protocol at <span class="span1">euler</span></h2>

						<p>• compute euler inception of cyber protocol based on Satoshi lottery and CBD balances</p>
						<p>• def knowledge graph state</p>
						<p>• take cyberlinks</p>
						<p>• check the validity of signatures</p>
						<p>• check bandwidth limit</p>
						<p>• check the validity of CIDv0</p>
						<p>• if signatures, bandwidth limit, and CIDv0 are ok than cyberlink is valid</p>
						<p>• for every valid cyberlink emit prediction as an array of CIDv0</p>
						<p>• every round calculate cyber•rank deltas for the knowledge graph</p>
						<p>• every round distribute CYB based on defined rules</p>
						<p>• apply more secure consensus state based on CBD balances 6 times up to merkle</p>
		

					</div>	

				</section>
				




				
				<section id="Knowledge_graph" class="contact">	
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Knowledge graph</h2>

						<p>We represent a knowledge graph as a weighted graph of directed links between content addresses or content
						identifications or CIDs. In this paper, we will use them as synonyms.</p>
						<div class='imgtwo'></div>
						<p>Content addresses are essentially a web3 links. Instead of using nonobvious and mutable thing:</p><br>
						<p class="fsize bormar">github.com/cosmos/cosmos/blob/master/WHITEPAPER.md</p><br>
						<p>we can use pretty much exact thing:</p><br>
						<p class="bormar fsize">Qme4z71Zea9xaXScUi6pbsuTKCCNFp5TAv8W5tjdfH7yuHhttps</p><br>
						<p>Using content addresses for building a knowledge graph we get <a class="linkcolor" href="https://steemit.com/web3/@hipster/an-idea-of-decentralized-search-for-web3-ce860d61defe5est" target="_blank">so much needed</a> superpowers of <a class="linkcolor" href="https://ipfs.io/ipfs/QmV9tSDx9UiPeWExXEeH6aoDvmihvx6jD5eLb4jbTaKGps" target="_blank">ipfs</a>-<a class="linkcolor" href="https://ipfs.io/ipfs/QmXHGmfo4sjdHVW2MAxczAfs44RCpSeva2an4QvkzqYgfR" target="_blank">like</a> p2p protocols for
						a search engine</p><br>
						<p class="bormar">• mesh-network future proof</p>
						<p class="bormar">• interplanetary</p>
						<p class="bormar">• tolerant</p>
						<p class="bormar">• accessible</p>
						<p class="bormar">• technology agnostic</p><br>
						<p>Web3 agents generate our knowledge graph. Web3 agents include itself to the knowledge graph by transacting only once.
						Thereby they prove the existence of private keys for content addresses of revealed public keys.</p>
						<p>Our euler implementation is based on <a class="linkcolor" href="https://github.com/cosmos/cosmos-sdk" target="_blank">cosmos-sdk</a> identities and <a class="linkcolor" href="https://github.com/multiformats/cid#cidv0" target="_blank">cidv0</a> content addresses.</p>
						<p>Web 3 agents generate knowledge graph by applying cyberlinks.</p>


				</section>
				




				<section id="Cyberlinks" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Cyberlinks</h2>

						<p>To understand cyberlinks, we need to understand the difference between URL link and IPFS link. URL link points to the
						location of content, but IPFS link point to the content itself. The difference in web architecture based on location
						links and content links is drastical, hence require new approaches.</p><br>
						<p>	Cyberlink is an approach to link two content addresses semantically.</p><br>
						<p class="fsize bormar">QmdvsvrVqdkzx8HnowpXGLi88tXZDsoNrGhGvPvHBQB6sH. QmdSQ1AGTizWjSRaVLJ8Bw9j1xi6CGLptNUcUodBwCkKNS</p><br>
						<p>This cyberlink means that cyberd presentation on cyberc0n is referencing Tezos whitepaper.</p><br>
						<p>A concept of cyberlink is a convention around simple semantics of communication format in any peer to peer network:</p><br>
						<p class="bormar">&#60;content-address x&#62;.&#60;content-address y&#62; &#60;/content-address&#62;</p><br>
						<p>You can see that cyberlink represents a link between two links. Easy peasy!</p><br>
						<p>Cyberlink is a simple yet powerful semantic construction for building a predictive model of the universe.</p><br>
						<p>Cyberlinks can form link chains if exist a series of two cyberlinks from one agent in which the second link in the
						first cyberlink is equal to the first link in the second cyberlink:</p><br>
						<ul class="bormar">
							<li>&#60;content-address x&#62;.&#60;content-address y&#62; &#60;/content-address&#62;</li>
							<li>&#60;content-address y&#62;.&#60;content-address z&#62; &#60;/content-address&#62;</li>
						</ul><br>
						<p>Using this simple principle, all interacting agents can reach consensus around interpreting clauses. So link chains are
						helpful for interpreting rich communications around relevance.</p>



						<div class='imgthree'></div>
						<p>Also using the following link: <p class='fsize'>QmNedUe2wktW65xXxWqcR8EWWssHVMXm3Ly4GKiRRSEBkn</p> the one can signal the start and stop of
						execution in the knowledge graph.</p><br>
						<p>If web3 agents expand native IPFS links with something semantically richer as <a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/web3-vision.md" target="_blank">DURA</a> links than web3 agents can easier to
						reach consensus on the rules for program execution.</p><br>
						<p>Indeed, DURA protocol is a proper implementation of a cyberlinks concept.</p><br>
						<p><span class="span1">euler</span> implementation of cyberlinks based on DURA specification is available in .cyber app of browser cyb.</p><br>
						<p>Based on cyberlinks we can compute the relevance of subjects and objects in a knowledge graph. That is why we need a
						consensus computer.</p>

					</div>
				</section>
				



				
				<section id="Notion_of_consensus_computer" class="contact">	
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Notion of consensus computer</h2>

						<p>Consensus computer is an abstract computing machine that emerges from agents interactions.</p><br>
						<p>A consensus computer has a capacity in terms of fundamental computing resources such as memory and computing. To
						interact with agents, a computer needs a bandwidth.</p><br>
						<p>Ideal consensus computer is a computer in which:</p><br>
						<ul class="bormar">
							<li>the sum of all *individual agents* computations and memory</li>
							<li>is equal to</li>
							<li>the sum of all verified by agents computations and memory of a *consensus computer*</li>
						</ul><br>
						<p>We know that:</p><br>
						<p class="bormar">verifications of computations < computations + verifications of computations</p><br>
						<p>Hence we will not be able to achieve an ideal consensus computer ever. CAP theorem and scalability trilemma also prove
						this statement.</p><br>
						<p>However, this theory can work as a performance indicator of a consensus computer.</p><br>
						<p>The euler implementation is a 64-bit consensus computer of the relevance for 64-byte string space that is as far from
						ideal at least as 1/146.</p><br>
						<p>We must bind computational, storage and bandwidth supply of relevance machine with maximized demand of queries.
						Computation and storage in case of basic relevance machine can be easily predicted based on bandwidth, but bandwidth
						requires a limiting mechanism.</p><br>
						<p>Bandwidth limiting mechanism is work in progress. Current notes on implementation are in <a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/bandwidth.md" target="_blank">the docs</a>.</p><br>
						<p>So agents must have CYB tokens in accordance to their will of learning the knowledge graph. However, proposed mechanics
						of CYB tokens work not only as spam protection but as the economic regulation mechanism to align the ability of
						validators to process knowledge graph and market demand for processing.</p><br>

							

					</div>
				</section>
				

				

				
				<section id="Relevance_machine" class="contact">
					
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Relevance machine</h2>

						<p>Relevance machine is a machine that transition knowledge graph state based on some reputation score of agents.</p><br>
						<p>	This machine enables simple construction for search question querying and answers delivering.</p><br>
						
						<p>
							The reputation score is projected on every agent's cyberlink. A simple rule prevents agents abuse: one content address
							can be voted by a token only once. So it does not matter for ranking from how much accounts you voted. The only sum of
							their balances matters.
						</p><br>
						
						<p>A useful property of a relevance machine is that it must have inductive reasoning property or follows the blackbox
						principle.</p><br>
						<ul class="bormar">
							<li>She must be able to interfere predictions</li>
							<li>without any knowledge about objects</li>
							<li>except who linked, when linked and what was linked.</li>
						</ul><br>
						
						<p>If we assume that a consensus computer must have some information about linked objects the complexity of such model
						growth unpredictably, hence a requirement for a computer for memory and computations. That is, deduction of meaning
						inside consensus computer is expensive thus our design depends on the blindness assumption. Instead of deducting a
						meaning inside consensus computer we design a system in which meaning extraction is incentivized because agents need
						CYB to compute relevance.</p><br>
						<p>Also, thanks to content addressing the relevance machine following the blackbox principle do not need to store the data
						but can effectively operate on it.</p><br>
						<p>Human intelligence organized in a way to prune none-relevant and none-important memories with time has passed. The same
						way can do relevance machine</p><br>
							<p>Also, one useful property of relevance machine is that it needs to store neither past state nor full current state to
							remain useful, or more precisely: relevant.</p><br>
								<p>So relevance machine can implement <a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/QmP81EcuNDZHQutvdcDjbQEqiTYUzU315aYaTyrVj6gtJb" target="_blank">aggressive pruning strategies</a> such as pruning all history of knowledge graph
								formation or forgetting links that become non-relevant.</p><br>
									<p>The pruning group of features can be implemented in nash.</p><br>
										<p>euler implementation of relevance machine is based on the most straightforward mechanism which is called cyber•Rank.</p>

					</div>
				</section>
			



				
				<section id="cyber•Rank" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;cyber•Rank</h2>

						<p>Ranking using consensus computer is hard because consensus computers bring serious resource bounds. e.g. <a class="linkcolor" href="https://ipfs.io/ipfs/QmWTZjDZNbBqcJ5b6VhWGXBQ5EQavKKDteHsdoYqB5CBjh" target="_blank">Nebulas</a> still
						fail to deliver something useful on-chain. First, we must ask ourselves why do we need to compute and store the rank
						on-chain, and not go <a class="linkcolor" href="https://ipfs.io/ipfs/QmZo7eY5UdJYotf3Z9GNVBGLjkCnE1j2fMdW2PgGCmvGPj" target="_blank">Colony</a> or <a class="linkcolor" href="https://ipfs.io/ipfs/QmTrxXp2xhB2zWGxhNoLgsztevqKLwpy5HwKjLjzFa7rnD" target="_blank">Truebit</a> way?</p><br>
						
						
						<p>If rank computed inside consensus computer, you have an easy content distribution of the rank as well as an easy way to
						build provable applications on top of the rank. Hence we decided to follow more cosmic architecture. In the next
						section, we describe the proof of relevance mechanism which allows the network to scale with the help of
						domain-specific relevance machines that works in parallel.</p><br>
						
						<p>Eventually, relevance machine needs to find (1) deterministic algorithm that allows computing a rank for a continuously
						appended network to scale the consensus computer to orders of magnitude that of Google. Perfect algorithm (2) must have
						linear memory and computation complexity. The most importantly it must have (3) highest provable prediction
						capabilities for the existence of relevant links.</p><br>
						
						<p>
						After some research, we found that we can not find silver bullet here. We find an algorithm that probably satisfies our
						criteria: SpringRank. An original idea of the algorithm came to Caterina from physics. Links represented as a system of
						springs with some energy, and the task of computing the ranks is the task of finding a relaxed state of springs.
						</p><br>
						
						<p>However, we got at least 3 problems with SpringRank:</p><br>
						<ul class="namber bormar">
							<li>We were not able to implement it on-chain fast using Go in <span class="span1">euler</span>.</li>
							<li>We were not able to prove it for knowledge graph because we did not have provable knowledge graph yet.</li>
							<li>Also, we were not able to prove it by applying it for the Ethereum blockchain during computing the genesis file for
							euler. It could work, but for the time being it is better to call this kind of distribution a lottery.</li>
						</ul><br>
						<p>So we decided to find some more basic bulletproof way to bootstrap the network: a rank from which Lary and Sergey have
						bootstrapped a previous network. The problem with original PageRank is that it is not resistant to sybil attacks.</p><br>

						<p>Token weighted <a class="linkcolor" href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf" target="_blank">PageRank</a> limited by token-weighted bandwidth do not have inherent problems of naive PageRank and is
						resistant to sybil attacks. For the time being, we will call it cyber•Rank until something better emerge.</p><br>
						<p>In the centre of spam protection system is an assumption that write operations can be executed only by those who have a
						vested interest in the evolutionary success of a relevance machine. Every 1% of stake in consensus computer gives the
						ability to use 1% of possible network bandwidth and computing capabilities.</p><br>
						<p>As nobody uses all possessed bandwidth, we can safely use 10x fractional reserves with 2-minute recalculation target.</p><br>
						<p>We would love to discuss the problem of vote buying mainly. Vote buying by itself is not such bad. The problem with
						vote buying appears in the systems where voting affects the allocation of inflation in the system like <a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/QmepU77tqMAHHuiSASUvUnu8f8ENuPF2Kfs97WjLn8vAS3" target="_blank">Steem</a> or any
						state-based system. So vote buying can become easily profitable for adversary employing a zero-sum game without a
						necessity to add value. Our original idea of a decentralized search was based on this approach, but we reject this idea
						completely removing incentive on consensus level for knowledge graph formation completely. In our setting in which
						every participant must bring some value to the system to affect predictive model vote buying become NP-hard problem
						hence is useful for the system.</p><br>
						<p>To switch from one algorithm to another, we are going to make simulations and experiment with economic a/b testing
						based on winning chains through hard spoons.</p><br>
						<p>Consensus computer based on relevance machine for cyber•Rank can answer and deliver relevant results for any given
						search request in the 64 byte CID space. However, to build a network of domain-specific relevance machines, it is not
						enough. Consensus computers must have the ability to prove relevance for each other.</p>
						
			
					</div>
				</section>
				


				<section id="Proof_of_relevance" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Proof of relevance</h2>

						<p>Proof of relevance
						We design a system under the assumption that regarding search such thing as bad behaviour does not exist as anything
						bad can be in the intention of finding answers. Also, this approach significantly reduces attack surfaces.</p><br>
						
						<div class="bordergrey"><p class="bormar">Ranks are computed on the only fact that something has been searched, thus linked and as a result, affected the
						predictive model.</p></div><br>
						
						<p>
							A good analogy is observing in quantum mechanics. That is why we do not need such things as negative voting. Doing this
							we remove subjectivity out of the protocol and can define proof of relevance.
						</p><br>
						
						<p class="bormar">Rank state = rank values stored in a one-dimensional array and merkle tree of those values</p><br>
						
						<p>
							Each new CID gets a unique number. The number starts from zero and incrementing by one for each new CID. So that we can
							store rank in a one-dimensional array where indices are CID numbers.
						</p><br>
						<p>Merkle Tree calculated based on <a class="linkcolor" href="https://tools.ietf.org/html/rfc6962#section-2.1" target="_blank">RFC-6962 standard</a>. Since rank stored in a one-dimensional array where indices are CID
						numbers (we could say that it ordered by CID numbers) leaves in merkle tree from left to right are SHA-256 hashes of
						rank value. Index of the leaf is CID number. It helps to easily find proofs for specified CID (log n iterations where n
						is a number of leaves).</p><br>
						<p>To store merkle tree is necessary to split the tree into subtrees with a number of leaves multiply of the power of 2.
						The smallest one is obviously subtree with only one leaf (and therefore height == 0). Leaf addition looks as follows.
						Each new leaf is added as subtree with height == 0. Then sequentially merge subtrees with the same height from right to
						left</p><br>
						<p>Example:</p><br>
						<p></p><br>
						<div>
								<p><img class="grafic" src="images/scheme-01.svg"></p>
						</div>	



						<p>To get merkle root hash - join subtree roots from right to left.</p><br>
						<p>Rank merkle tree can be stored differently:</p><br>
						<p>Full tree - all subtrees with all leaves and intermediary nodes</p>
						<p>Short tree - contains only subtrees roots</p><br>
						<p>The trick is that full tree is only necessary for providing merkle proofs. For consensus purposes and updating tree,
						it's enough to have a short tree. To store merkle tree in database use only a short tree. Marshaling of a short tree
						with n subtrees (each subtree takes 40 bytes):</p><br>
						<div class="bormar">
							<p>&#60;subtree_1_root_hash_bytes&#62;
									&#60;subtree_1_height_bytes&#62;</p>
							<p>....</p>
							<p>&#60;subtree_n_root_hash_bytes&#62;
									&#60;subtree_n_height_bytes&#62;</p>
						</div><br>
						<p>For 1,099,511,627,775 leaves short tree would contain only 40 subtrees roots and take only 1600 bytes.</p><br>
						<p>Let us denote rank state calculation:</p><br>
						<p><span class="span1">p</span> - rank calculation period</p>
						<p><span class="span1">lbn</span> - last confirmed block number</p>
						<p><span class="span1">cbn</span> - current block number</p>
						<p><span class="span1">lr</span> - length of rank values array</p><br>
						<p>For rank storing and calculation we have two separate in-memory contexts:</p><br>
						<ul class="bormar namber">
							<li>Current rank context. It includes the last calculated rank state (values and merkle tree) plus all links and user
							stakes submitted to the moment of this rank submission.</li>
							<li>New rank context. It's currently calculating (or already calculated and waiting for submission) rank state. Consists of
							new calculated rank state (values and merkle tree) plus new incoming links and updated user stakes.</li>
						</ul><br>
						<p>Calculation of new rank state happens once per p blocks and going in parallel.</p><br>
						<p>The iteration starts from block number that ≡ 0 (mod p) and goes till next block number that ≡ 0 (mod p).</p><br>
						<p>For block number cbn ≡ 0 (mod p) (including block number 1 cause in cosmos blocks starts from 1):</p><br>
						<ul class="bormar namber">
							<li>Check if the rank calculation is finished. If yes then go to (2.) if not - wait till calculation finished (actually
							this situation should not happen because it means that rank calculation period is too short).</li>
							<li>Submit rank, links and user stakes from new rank context to current rank context.</li>
							<li>Store last calculated rank merkle tree root hash.</li>
							<li>Start new rank calculation in parallel (on links and stakes from current rank context).</li>
						</ul><br>
						<p>For each block:</p><br>
						<ul class="bormar namber">
							<li>All links go to a new rank context.</li>
							<li>New coming CIDs gets rank equals to zero. We could do it by checking last CIDs number and lr (it equals the number of
							CIDs that already have rank). Then add CIDs with number >lr to the end of this array with the value equal to zero.</li>
							<li>Update current context merkle tree with CIDs from the previous step</li>
							<li>Store latest merkle tree from current context (let us call it last block merkle tree).</li>
							<li>Check if new rank calculation finished. If yes go to (4.) if not go to next block.</li>
							<li>Push calculated rank state to new rank context. Store merkle tree of newly calculated rank.</li>
						</ul><br>
						<p>To sum up. In current rank context, we have rank state from last calculated iteration (plus, every block, it updates
						with new CIDs). Moreover, we have links and user stakes that are participating in current rank calculation iteration
						(whether it finished or not). The new rank context contains links and stakes that will go to next rank calculation and
						newly calculated rank state (if a calculation is finished) that waiting for submitting.</p><br>
						<p>If we need to restart node firstly, we need to restore both contexts (current and new). Load links and user stakes from
						a database using different versions:</p><br>
						<ul class="bormar namber">
							<li>Links and stakes from last calculated rank version v = lbn - (lbn mod n) go to current rank context.</li>
							<li>Links and stakes between versions v and lbn go to new rank context.</li>
						</ul><br>
						<p>Also to restart node correctly, we have to store following entities in database:</p><br>
						<ul class="bormar namber">
							<li>Last calculated rank hash (merkle tree root)</li>
							<li>A newly calculated rank short merkle tree</li>
							<li>Last block short merkle tree</li>
						</ul><br>
						<p>With last calculated rank hash and newly calculated rank merkle tree we could check if the rank calculation was
						finished before node restart. If they are equal, then rank wasn't calculated, and we should run the rank calculation.
						If not we could skip rank calculation and use newly calculated rank merkle tree to participate in consensus when it
						comes to block number cbn ≡ 0 (mod p) (rank values will not be available until rank calculation happens in next
						iteration. Still validator can participate in consensus so nothing bad).</p><br>
						<p>Last block merkle tree necessary to participate in consensus till the start of next rank calculation iteration. So,
						after the restart we could end up with two states:</p><br>
						<ul class="bormar namber">
							<li>Restored current rank context and new rank context without rank values (links, user stakes, and merkle tree).</li>
							<li>Restored current rank context without rank values. Restored new rank context only with links and user stakes.</li>
						</ul><br>
						<p>A node can participate in consensus but cannot provide rank values (and merkle proofs) till two rank calculation
						iterations finished (current and next).</p><br>
						<p>Search index should be run in parallel and do not influence the work of the consensus machine. The validator should be
						able to turn off index support. Maybe even make it a separate daemon.</p><br>
						<p>Base idea. Always submit new links to index and take rank values from current context (insert in sorted array
						operation). When a new rank state is submitted trigger index to update rank values and do sortings (in most cases new
						arrays will be almost sorted).</p><br>
						<p>Need to solve the problem of adjusting arrays capacity (not to copy arrays each time newly linked cid added). A
						possible solution is to adjust capacity with reserve before resorting array.</p><br>
						<p>Todo: Therefore for building index, we need to find a sorting algorithm that will be fast on almost sorted arrays.
						Also, we should implement it for GPU so it should better be parallelizable: Mergesort(Timsort), Heapsort, Smoothsort
						...</p><br>
						<p>Now we have proof of rank of any given content address. While the relevance is still subjective by nature, we have a
						collective proof that something was relevant for some community at some point in time.</p><br>
						<p class="bormar">For any given CID it is possible to prove the relevance</p><br>
						<p>Using this type of proof any two <a class="linkcolor" href="https://ipfs.io/ipfs/QmdCeixQUHBjGnKfwbB1dxf4X8xnadL8xWmmEnQah5n7x2" target="_blank">IBC compatible</a> consensus computers can proof the relevance to each other so that
						domain-specific relevance machines can flourish. Thanks to inter-blockchain communication protocol you basically can
						launch your own domain-specific search engine either private or public by forking cyberd which is focused on the common
						public knowledge. So in our search architecture, domain-specific relevance machine can learn from common knowledge. We
						are going to work on IBC during smith implementation.</p><br>
						<div class="imgfive"></div>
						<p>In our relevance for commons euler implementation proof of relevance root hash is computed on Cuda GPUs every round.</p>

					

					</div>
				</section>


				<section id="Speed_and_scalability" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Speed and scalability</h2>

						<p>
						We need speedy confirmation times to feels like the usual web app. It is a strong architecture requirement that shape
						an economic topology and scalability of the cyber protocol.
						</p><br>
						<p>Proposed blockchain design is based on <a class="linkcolor" href="https://ipfs.io/ipfs/QmaMtD7xDgghqgjN62zWZ5TBGFiEjGQtuZBjJ9sMh816KJ" target="_blank">Tendermint consensus</a> algorithm with 146 validators and has very fast 1 second
						finality time. Average confirmation timeframe at half the second with asynchronous interaction make complex blockchain
						search almost invisible for agents.</p><br>
						<p>Let us say that our node implementation based on cosmos-sdk can process 10k transactions per second. Thus every day at
						least 8.64 million agents can submit 100 cyberlinks each and impact results simultaneously. That is enough to verify
						all assumptions in the wild. As blockchain technology evolves we want to check that every hypothesis work before scale
						it further. Moreover, proposed design needs demand for full bandwidth in order the relevance become valuable. That is
						why we strongly focus on accessible, but provable distribution to millions from inception.</p>

					</div>
				</section>


				<section id="Implementation_in_a_browser" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Implementation in a browser</h2>

						<p>Implementation in a browser
						We wanted to imagine how that could work in a web3 browser. To our disappointment we <a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/comparison.md" target="_blank">was not able</a> to find the web3
						browser that can showcase the coolness of the proposed approach in action. That is why we decide to develop the web3
						browser <a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/cyb.md" target="_blank">cyb</a> that has sample application .cyber for interacting with cyber:// protocol.</p><br>
						<div class="imgscrin"></div>
						<div class="imgscrinn"></div>
						
						

					</div>
				</section>


				<section id="From_Inception_to_Genesis" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;From Inception to Genesis</h2>

						<p>
						It is trivial to develop euler like proof-of-concept implementation, but it is hard to achieve stable protocol merkle a
						lot of CYB value on which can exist. euler is Inception that already happened, merkle is Genesis that is far away. That
						is why we decide to innovate a bit on the going main net process. We do not have CYB balances and rank guaranties
						before merkle but we can have exponentially growing semantic core which can be improved based on measurements and
						observations during development and gradual transfer of value since euler. So think that Genesis or merkle is very
						stable and can store semantic core and value, but all releases before can store the whole semantic core and only part
						of the value you would love to store due to weak security guarantees. The percents of CYB value to be distributed based
						on CBD balances:
						</p>
						<br>
						<p>euler = 1</p>
						<p>smith = 4</p>
						<p>darwin = 8</p>
						<p>turing = 15</p>
						<p>nash = 21</p>
						<p>weiner = 25</p>
						<p>merkle = 27</p><br>
						<p>To secure the value of CYB before Genesis 100 CBD ERC-20 tokens <a href="https://etherscan.io/token/0x136c1121f21c29415D8cd71F8Bb140C7fF187033" class="linkcolor" target="_blank">are issued</a> by <a class="linkcolor" href="https://mainnet.aragon.org/#/cyberfoundation.aragonid.eth/0xf4d85b5a1650a335b30072d178f6dcb611f05a3e" target="_blank">cyberFoundation</a>. So snapshot balances are
						computed 7 times based on CBD.</p><br>
						<p>Essentially CBD substance is distributed by cyberFoundation in the following proportion:</p><br>
						<ul class="disc bormar">
							<li>Proof-of-use: 70% is allocated to web3 agents according to some probabilistic algorithm. E.g., first euler proof-of-use
							distribution we call Satoshi Lottery is allocated to key owned Ethereum addresses based on ongoing research. First
							allocation is based on SpringRank.</li>
							<li>Proof-of-code: 15% is allocated for direct contribution to the code base. E.g., as assigned by cyberFoundation to
							cyberCongress contribution including team is 11.2% and the other 3.8% allocated to developers community projects such
							as Gitcoin community and cyberColony based experimental organization.</li>
							<li>Proof-of-value: 15% is allocated for a direct contribution of funds. 8% of this value either has been already
							contributed nor has some reservation for ongoing contributions by close friends and 7% is going to be distributed
							during Eos-like auction not defined precisely yet. All contribution from the auction will go to Aragon based
							cyberFoundation and will be managed by CBD token holders.</li>
						</ul><br>
						<p>Details of code and value distribution can be produced by cyberFoundation.</p><br>
						<p>Except for 7 CBD based distributions, CYB tokens can be created only by validators based on staking and slashing
						parameters. The basic consensus is that newly created CYB tokens are distributed to validators as they do the essential
						work to make relevance machine run both regarding energy consumed for computation and cost for storage capacity. So
						validators decide where the tokens can flow further.</p>

					</div>
				</section>


				<section id="Validators_incentive" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Validators incentive</h2>

						<p>
							Validators are the essential building block of the proposed architecture. Hence we want to bring them a better
							incentive to participate before the main net. In our case validators will compute and process requests for billions
							edge knowledge graph hence it would be naive to expect that it is possible to expect to prepare such a network for
							production for free. In the beginning, inflation must be high enough to compensate risks of early investments into the
							ecosystem.
						</p><br>
						<p>This is approximation of year inflation expressed in percents defined for testnets:</p><br>
						<p>euler = 200</p>
						<p>smith = 134</p>
						<p>darwin = 90</p>
						<p>turing = 60</p>
						<p>nash = 40</p>
						<p>weiner = 27</p>
						<p>merkle = 18</p><br>
						<p>The scheme motivates developers to release earlier to be less diluted from holding CBD and honour validators if
						development is going slower than expected.</p><br>
						<p>After Genesis starting inflation rate will become fixed at 1 000 000 000 CYB per block.</p><br>
						<p><a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/run_validator.md" target="_blank">Join.</a></p><br>
						<p>Once we have validators, we can think about first million web3 agents.</p>

					</div>
				</section>


				<section id="Satoshi_Lottery" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Satoshi Lottery</h2>

							<p>
								Satoshi Lottery is the inception version of the proof-of-use distribution that already happens in the tenth birthday of
								Bitcoin Genesis at 3 Jan 2019. It is a highly experimental way of provable distribution. The basic idea is that a
								comprehensive set of agents receive CYB tokens because they behave well. The basic algorithm is of 5 steps:
							</p><br>
							<ul class="bormar">
								<li>- Compute SpringRank for Ethereum addresses</li>
									<li>- Sort by SpringRank</li>
										<li>- Filter top 1M addresses by SpringRank</li>
											<li>- Compute CYB balances based on CBD</li>
												<li>- Create genesis for cyber protocol</li>
							</ul><br>
							<p>Translation todo: <a class="linkcolor" href="https://ipfs.io/ipfs/QmS4YuH377EyzAjH84AR4EZKrnT879pMz4VKXcDNuej9DZ" target="_blank">Tolik's article</a> have to be translated here.</p><br>
							<p>Next test net we will improve the logic of the lottery based on received data and repeat this every test net until
							Genesis.</p><br>
							<p>Soon you will be able to verify either you were lucky enough to receive CYB or not just searching your ethereum
							address. If you were, you will be able to claim CYB even without compromising your Ethereum keys.</p>

					</div>
				</section>


				<section id="Inception" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Inception</h2>

						<p>The genesis file for euler containing lottery results and CBD based distribution has the following cid:</p><br>
						<p class="fsize bormar">Qma5U4joYWEf41ku16g9cQr6fADsxCPsiWeYZBxpnpu1D4</p><br>
						<p>132307 accounts with 8 274 000 000 000 000 CYB tokens has been created in Inception of the network.</p><br>
						<p>Amount of created tokens is consist of the following sources:</p><br>
						<ul class="bormar disc">
							<li>1% of CYB value allocated to euler testnet based on proof-of-use distribution as planned</li>
							<li>0.7% of CYB value allocated to euler testnet based on proof-of-value and proof-of-code distribution except 11.8 CBD due
							to bug. Appropriate corrections will be done during scheduled hardfork.</li>
						</ul>
					</div>
				</section>


				<section id="Possible_applications" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Possible applications</h2>

						<p>
							A lot of cool applications can be built on top of proposed architecture:
						</p><br>
						<p>
						Web3 browsers. It easy to imagine the emergence of a full-blown blockchain browser. Currently, there are several
						efforts for developing browsers around blockchains and distributed tech. Among them are Beaker, Mist, Brave, and
						Metamask. All of them suffer from trying to embed web2 in web3. Our approach is a bit different. We consider web2 as
						the unsafe subset of web3. That is why we decide to develop a web3 browser that can showcase the cyber approach to
						answer questions better.
						</p><br>
						<p>Programmable semantic cores. Currently, the most popular keywords in a gigantic semantic core of Google are keywords of
						apps such as youtube, facebook, github, etc. However, developers have very limited possibility to explain Google how to
						better structure results. The cyber approach brings this power back to developers. On any given user input string in
						any application relevant answer can be computed either globally, in the context of an app, a user, a geo or in all of
						them combined.</p><br>
						<p>Search actions. Proposed design enable native support for blockchain asset related activity. It is trivial to design
						applications which are (1) owned by creators, (2) appear right in search results and (3) allow a transact-able call to
						actions with (4) provable attribution of a conversion to search query. e-Commerce has never been so easy for everybody.</p><br>
						<p>Offline search. IPFS make possible easy retrieval of documents from surroundings without a global internet connection.
						cyberd can itself can be distributed using IPFS. That creates a possibility for ubiquitous offline search.</p><br>
						<p>Command tools. Command line tools can rely on relevant and structured answers from a search engine. That practically
						means that the following CLI tool is possible to implement</p><br>


						<div class="bormar">
							<p>&#62;cyberd earn using 100 gb hdd</p><br>
							<p>Enjoy the following predictions:</p>
							<ul>
								<li>- apt install go-filecoin:&#8194;&#8195; 0.001 BTC per month per GB</li>
								<li>- apt install siad:&#160;&#160;&#160;&#8195;&#8195;&#8195;&#8195;0.0001 BTC per month per GB</li>
								<li>- apt install storjd:&#160;&#8195;&#8195;&#8195;&#8194; 0.00008 BTC per month per GB</li>
							</ul><br>
							<p>According to the best prediction, I made a decision try `mine go-filecoin`</p><br>
							<p>Git clone ...</p>
								<p>Building go-filecoin</p>
									<p>Starting go-filecoin</p>
										<p>Creating a wallet using @xhipster seed</p>
											<p>You address is ....</p>
												<p>Placing bids ...</p>
													<p>Waiting for incoming storage requests ...</p><br>
						</div>
						<p>Search from CLI tools will inevitably create a highly competitive market of a dedicated semantic core for bots.</p><br>
						<p>Autonomous robots. Blockchain technology enables the creation of devices which can earn, store, spend and invest
						digital assets by themselves.</p><br>
						<div class="bordergrey">
							<p class="bormar">If a robot can earn, store, spend and invest she can do everything you can do</p>
						</div><br>
						<p>What is needed is a simple yet powerful state reality tool with the ability to find particular things. cyberd offers
						minimalistic but continuously self-improving data source that provides necessary tools for programming economically
						rational robots. According to <a class="linkcolor" href="https://github.com/first20hours/google-10000-english" target="_blank">top-10000 english words</a> the most popular word in English is defined article the that
						means a pointer to a particular thing. That fact can be explained as the following: particular things are the most
						important for us. So the nature of our current semantic computing is to find unique things. Hence the understanding of
						unique things become essential for robots too.</p><br>
						<p>Language convergence. A programmer should not care about what language do the user use. We don't need to know about
						what language user is searching in. Entire UTF-8 spectrum is at work. A semantic core is open so competition for
						answering can become distributed across different domain-specific areas, including semantic cores of different
						languages. The unified approach creates an opportunity for cyber•Bahasa. Since the Internet, we observe a process of
						rapid language convergence. We use more truly global words across the entire planet independently of our nationality,
						language and race, Name the Internet. The dream of truly global language is hard to deploy because it is hard to agree
						on what means what. However, we have the tools to make that dream come true. It is not hard to predict that the shorter
						a word, the more its cyber•rank will be. Global publicly available list of symbols, words, and phrases sorted by
						cyber•rank with corresponding links provided by cyberd can be the foundation for the emergence of genuinely global
						language everybody can accept. Recent <a class="linkcolor" href="https://ipfs.io/ipfs/QmQUWBhDMfPKgFt3NfbxM1VU22oU8CRepUzGPBDtopwap1" target="_blank">scientific advances</a> in machine translation are breathtaking but meaningless for
						those who wish to apply them without Google scale trained model. Proposed cyber•rank offers precisely this.</p><br>
						<p>This is sure not the exhaustive list of possible applications but very exciting, though.</p>							


					</div>
				</section>


				<section id="Economic_protection_is_smith" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Economic protection is smith</h2>

						<p>About private knowledge on relevance. Explain the difference between private cyberlinks and private relevance machines.</p><br>
						<p>The plan for learning the beast. How cyberlink ipfs, wiki, bitcoin and ethereum?</p>

					</div>
				</section>


				<section id="Ability_to_evolve_is_darwin" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Ability to evolve is darwin</h2>

						<p>About the importance of alternative implementation.</p>

					</div>
				</section>


				<section id="Turing_is_about_computing_more" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Turing is about computing more</h2>

						<p>
						Ability to programmatically extend state based on proven knowledge graph is of paramount importance. Thus we consider
						that WASM programs will be available for execution in cyber consensus computer on top of knowledge graph.
						</p>
						<br>
						<p>
						Our approach to the economics of consensus computer is that users buy an amount of RAM, CPU, and GPU as they want to
						execute programs. OpenCypher or GraphQL like language can be provided to explore semantics of the knowledge graph. The
						following list is simple programs we can envision that can be built on top of simple relevance machine.
						</p><br>
						<p>
							Self prediction. A consensus computer can continuously build a knowledge graph by itself predicting the existence of
							cyberlinks and applying these predictions to a state of itself. Hence a consensus computer can participate in the
							economic consensus of the cyber protocol.
						</p><br>
						<p>
						Universal oracle. A consensus computer can store the most relevant data in the key-value store, where the key is cid
						and value is bytes of actual content. She is doing it by making a decision every round about which cid value she want
						to prune and which she wants to apply based on the utility measure of content addresses in the knowledge graph. To
						compute utility measure validators check availability and size of content for the top-ranked content address in the
						knowledge graph, then weight on the size of cids and its ranks. The emergent key-value store will be available to write
						for consensus computer only and not agents, but values can be used in programs.
						</p><br>
						<p>
						Proof of location. It is possible to construct cyberlinks with proof-of-location based on some existing protocol such
						as <a class="linkcolor" href="https://ipfs.io/ipfs/QmZYKGuLHf2h1mZrhiP2FzYsjj3tWt2LYduMCRbpgi5pKG" target="_blank">Foam.</a> So location-based search also can become provable if web3 agents will mine triangulations and attaching proof
						of location for every link chain.
						</p><br>
						
					
						<p>Proof of web3 agent. Agents are a subset of content addresses with one fundamental property: consensus computer can
						prove the existence of private keys for content addresses for the subset of knowledge graph even if those addresses has
						never transacted in its own chain. Hence it is possible to compute much provable stuff on top of that knowledge. E.g.,
						some inflation can be distributed to addresses that have never transacted in the cyber network but have the provable
						link.</p><br>
						<p>
						Motivation for read requests. It would be great to create cybernomics not only for write requests to consensus computer
						but from read requests also. So read requests can be two order of magnitude cheaper, but guaranteed. Read requests to a
						search engine can be provided by the second tier of nodes which earn CYB tokens in state channels. We consider
						implementing state channels based on HTLC and proof verification which unlocks amount earned for already served
						requests.
						</p><br>
					
						<p>
						Prediction markets on link relevance. We can move the idea further by the ranking of knowledge graph based on
						prediction market on links relevance. An app that allow betting on link relevance can become a unique source of truth
						for the direction of terms as well as motivate agents to submit more links.
						</p><br>
						<p>
						Private cyberlinks. Privacy is foundational. While we are committed to privacy achieving implementation of private
						cyberlinks is unfeasible for our team up to Genesis. Hence it is up to the community to work on wasm programs that can
						be executed on top of the protocol. The problem is to compute cyberRank based on cyberlink submitted by a web3 agent
						without revealing neither previous request nor public keys of a web3 agent. Zero-knowledge proofs, in general, are very
						expensive. We believe that privacy of search should be must by design, but not sure that we know how to implement it.
						<a class="linkcolor" href="https://github.com/cybercongress/cyberd/tree/master/docs" target="_blank">Coda</a> like recursive snarks and <a class="linkcolor" href="https://github.com/cybercongress/cyberd/tree/master/docs" target="_blank">mimblewimble</a> constructions, in theory, can solve part of the privacy issue, but they are
						new, untested and anyway will be more expensive regarding computations than a transparent alternative.
						</p>
						

					</div>
				</section>


				<section id="In_a_search_for_equilibria_is_nash" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;In a search for equilibria is nash</h2>

						<p>
						We need to find answers for a lot of hard questions regarding consensus variables and its default values. So we decide
						to stick to a community generated feedback on the road to Genesis and continuously adjust them to keep going better.
						</p><br>
						<p>
						On scalability trilemma ...
						</p><br>
						<p>
						Decentralization comes with costs and slowness. We want to find a good balance between speed, reliance, and ability to
						scale, as we believe all three are sensitive for widespread web3 adoption.
						</p><br>
						<p>That is the area of research for us now. We need real economic measurements to apply a scientific method for this class
						of challenges.</p>

					</div>
				</section>


				<section id="On_faster_evolution_at_weiner" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;On faster evolution at weiner</h2>

						<p>
						The primary purpose of wiener stage is to be able to update the consensus of a network from a consensus computer state
						using some on-chain upgrade mechanism.
						</p><br>
						<p>Evolvability and governance are connected tightly.</p><br>
						<p>Ability to reflect input from the world and output changes of itself is an essential evolutionary feature. Hence,
						thanks to cosmos-sdk euler implementation have basic but compelling features such as on-chain voting with vetos and
						abstain that drastically simplified open discussions for a change. So we are going to use this feature from the
						inception of the network.</p><br>
						<p>However, we can go in a different direction than cosmos-sdk offers. Following ideas from <a class="linkcolor" href="https://ipfs.io/ipfs/QmdSQ1AGTizWjSRaVLJ8Bw9j1xi6CGLptNUcUodBwCkKNS" target="_blank">Tezos</a> in weiner we can define
						the current state of a protocol as the immutable content address that included in round merkle root.</p><br>
						<p>Also instead of formal governance procedure, we would love to check the hypothesis that changing state of a protocol is
						possible indeed using relevance machine itself.</p><br>
						<p>Starting protocol can be as simple as follows:</p><br>
						<div class="bordergrey">
							<p class="bormar fsize">The closer some content address to QmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh the more probability that it becomes
							the winning during an upgrade. The closest protocol to cyber-protocol-current is the protocol which is the most
							relevant to users.</p>
						</div><br>
						<p>Hence it is up to nodes to signal cyber-protocol-current by sending cyberlinks with semantics like</p>
						<p class="fsize">&#60;cQmRBKYsQ4FPEtHeGBRuUZEfNXQfvNiJFXvbyrdF4Y7pqfh&#62;
								&#60;cid-of-protocol&#62;.</p>

					</div>
				</section>


					<section id="Genesis_is_secure_as_merkle" class="contact">
						<div class="section-header">
							<h2><span>&#8260; </span>&nbsp;Genesis is secure as <span class="span1">merkle</span></h2>
					
							<p>We define and implement a protocol for provable communications of consensus computers on relevance. The protocol
								is
								based on a simple idea of content defined knowledge graphs which are generated by web3 agents using cyberlinks.
								Cyberlinks are processed by a consensus computer using a concept we call relevance machine. euler consensus computer
								is
								based on CIDv0 and uses go-ipfs and cosmos-sdk as a foundation. IPFS provide significant benefits regarding
								resources
								consumption. CIDv0 as primary objects are robust in its simplicity. For every CIDv0 cyber•rank is computed by a
								consensus computer with no single point of failure. Cyber•rank is CYB weighted PageRank with economic protection
								from
								sybil attacks and selfish voting. Every round merkle root of the rank tree is published so every computer can prove
								to
								any computer a relevance value for a given CID. Sybil resistance is based on bandwidth limiting. Embedded ability to
								execute programs offer inspiring apps. Starting primary goal is indexing of peer-to-peer systems with
								self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorrent, or stateful such as Bitcoin,
								Ethereum and other blockchains and tangles. Proposed semantics of linking offers a robust mechanism for predicting
								meaningful relations between objects by a consensus computer itself. The source code of a relevance machine is open
								source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to
								process it. The performance of proposed software implementation is sufficient for seamless user interactions.
								Scalability of proposed implementation is enough to index all self-authenticated data that exist today and serve it
								to
								millions of web3 agents. The blockchain is managed by a decentralized autonomous organization which functions under
								Tendermint consensus algorithm with standard governance module. Thought a system provide necessary utility to offer
								an
								alternative for conventional search engines it is not limited to this use case either. The system is extendable for
								numerous applications and, e.g. makes it possible to design economically rational self-owned robots that can
								autonomously understand objects around them.</p>
					
						</div>
					</section>


				<section id="Conclusion" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;Conclusion</h2>

						<p>We define and implement a protocol for provable communications of consensus computers on relevance. The protocol is
						based on a simple idea of content defined knowledge graphs which are generated by web3 agents using cyberlinks.
						Cyberlinks are processed by a consensus computer using a concept we call relevance machine. euler consensus computer is
						based on CIDv0 and uses go-ipfs and cosmos-sdk as a foundation. IPFS provide significant benefits regarding resources
						consumption. CIDv0 as primary objects are robust in its simplicity. For every CIDv0 cyber•rank is computed by a
						consensus computer with no single point of failure. Cyber•rank is CYB weighted PageRank with economic protection from
						sybil attacks and selfish voting. Every round merkle root of the rank tree is published so every computer can prove to
						any computer a relevance value for a given CID. Sybil resistance is based on bandwidth limiting. Embedded ability to
						execute programs offer inspiring apps. Starting primary goal is indexing of peer-to-peer systems with
						self-authenticated data either stateless, such as IPFS, Swarm, DAT, Git, BitTorrent, or stateful such as Bitcoin,
						Ethereum and other blockchains and tangles. Proposed semantics of linking offers a robust mechanism for predicting
						meaningful relations between objects by a consensus computer itself. The source code of a relevance machine is open
						source. Every bit of data accumulated by a consensus computer is available for everybody if the one has resources to
						process it. The performance of proposed software implementation is sufficient for seamless user interactions.
						Scalability of proposed implementation is enough to index all self-authenticated data that exist today and serve it to
						millions of web3 agents. The blockchain is managed by a decentralized autonomous organization which functions under
						Tendermint consensus algorithm with standard governance module. Thought a system provide necessary utility to offer an
						alternative for conventional search engines it is not limited to this use case either. The system is extendable for
						numerous applications and, e.g. makes it possible to design economically rational self-owned robots that can
						autonomously understand objects around them.</p>
					
					</div>
				</section>


				


				<section id="References" class="contact">
					<div class="section-header">
						<h2><span>&#8260; </span>&nbsp;References</h2>
						
						<ul class="disc bormar">
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyberd" target="_blank">cyberd</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmNhaUrhM7KcWzFYdBeyskoNyihrpHvUEBQnaddwPZigcN" target="_blank">Scholarly context adrift</a></li>
							<li><a class="linkcolor" href="https://github.com/w3f/Web3-wiki/wiki" target="_blank">Web3 stack</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmeS4LjoL1iMNRGuyYSx78RAtubTT2bioSGnsvoaupcHR6" target="_blank">Search engines information retrieval in practice</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmNrAFz34SLqkzhSg4wAYYJeokfJU5hBEpkT4hPRi226y9.ifps" target="_blank">Motivating game for adversarial example research</a></li>
							<li><a class="linkcolor" href="https://steemit.com/web3/@hipster/an-idea-of-decentralized-search-for-web3-ce860d61defe5est" target="_blank">An idea of decentralized search</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmV9tSDx9UiPeWExXEeH6aoDvmihvx6jD5eLb4jbTaKGps" target="_blank">IPFS</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmXHGmfo4sjdHVW2MAxczAfs44RCpSeva2an4QvkzqYgfR" target="_blank">DAT</a></li>
							<li><a class="linkcolor" href="https://github.com/cosmos/cosmos-sdk" target="_blank">cosmos-sdk</a></li>
							<li><a class="linkcolor" href="https://github.com/multiformats/cid#cidv0" target="_blank">CIDv0</a></li>
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/bandwidth.md" target="_blank">Bandwidth in cyber network</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmP81EcuNDZHQutvdcDjbQEqiTYUzU315aYaTyrVj6gtJb" target="_blank">Thermodynamics of predictions</a></li>
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/web3-vision.md" target="_blank">DURA</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmWTZjDZNbBqcJ5b6VhWGXBQ5EQavKKDteHsdoYqB5CBjh" target="_blank">Nebulas</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmZo7eY5UdJYotf3Z9GNVBGLjkCnE1j2fMdW2PgGCmvGPj" target="_blank">Colony</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmTrxXp2xhB2zWGxhNoLgsztevqKLwpy5HwKjLjzFa7rnD" target="_blank">Truebit</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmNvxWTXQaAqjEouZQXTV4wDB5ryW4PGcaxe2Lukv1BxuM" target="_blank">SpringRank</a></li>
							<li><a class="linkcolor" href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf" target="_blank">PageRank</a></li>
							<li><a class="linkcolor" href="https://tools.ietf.org/html/rfc6962#section-2.1" target="_blank">RFC-6962</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmdCeixQUHBjGnKfwbB1dxf4X8xnadL8xWmmEnQah5n7x2" target="_blank">IBC protocol</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmaMtD7xDgghqgjN62zWZ5TBGFiEjGQtuZBjJ9sMh816KJ" target="_blank">Tendermint</a></li>
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/comparison.md" target="_blank">Comparison of web3 browsers</a></li>
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyb/blob/master/docs/cyb.md" target="_blank">Cyb</a></li>
							<li><a class="linkcolor" href="https://etherscan.io/token/0x136c1121f21c29415D8cd71F8Bb140C7fF187033" target="_blank">CBD</a></li>
							<li><a class="linkcolor" href="https://mainnet.aragon.org/#/cyberfoundation.aragonid.eth/0xf4d85b5a1650a335b30072d178f6dcb611f05a3e" target="_blank">cyberFoundation in Aragon</a></li>
							<li><a class="linkcolor" href="https://github.com/cybercongress/cyberd/blob/master/docs/how_to_become_validator.md" target="_blank">How to become validator in cyber protocol</a></li>
							<li><a class="linkcolor" href="http://ipfs.io/ipfs/QmV2kjvY1QvY17sPgBDRGd13vaaks1zmFj867Ez5mbXz1y/genesis-explanation.html" target="_blank">Tolik's article on Satoshi Lottery</a></li>
							<li><a class="linkcolor" href="https://github.com/first20hours/google-10000-english" target="_blank">Top 10000 english words</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmQUWBhDMfPKgFt3NfbxM1VU22oU8CRepUzGPBDtopwap1" target="_blank">Multilingual neural machine translation</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmZYKGuLHf2h1mZrhiP2FzYsjj3tWt2LYduMCRbpgi5pKG" target="_blank">Foam</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/Qmdje3AmtsfjX9edWAxo3LFhV9CTAXoUvwGR7wHJXnc2Gk" target="_blank">Coda</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/Qmd99xmraYip9cVv8gRMy6Y97Bkij8qUYArGDME7CzFasg" target="_blank">Mimblewimble</a></li>
							<li><a class="linkcolor" href="https://ipfs.io/ipfs/QmdSQ1AGTizWjSRaVLJ8Bw9j1xi6CGLptNUcUodBwCkKNS" target="_blank">Tezos</a></li>
							

						</ul>
						

					</div>
				</section>



	</div>
		
<div>
	<p><img class="scrollup" src="images/uppp.svg" alt="uppp"></p>
</div>


</main>

	<script type="text/javascript" src="js/jquery-1.12.3.min.js"></script>
	<script type="text/javascript" src="js/jquery.onepage-scroll.min.js"></script>
	<script type="text/javascript" src="js/jquery.easing.min.js"></script>
	<script type="text/javascript" src="js/jquery.backstretch.min.js"></script>
	<script type="text/javascript" src="js/jquery.filterizr.js"></script>
	<script type="text/javascript" src="js/jquery.magnific-popup.min.js"></script>
	
	<script type="text/javascript" src="js/owl.carousel.min.js"></script>
	<script type="text/javascript" src="js/custom.js"></script>
	<script type="text/javascript" src="js/smoothscroll.min.js"></script>

</body>
</html>